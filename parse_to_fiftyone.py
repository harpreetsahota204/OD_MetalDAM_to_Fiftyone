import os
import requests
import zipfile
import fiftyone as fo
from PIL import Image
import pandas as pd
import sqlite3

def download_dataset():
    """
    Download the OD_MetalDAM Dataset from source
    """
    urls = [
        "https://github.com/ari-dasci/OD-MetalDAM/releases/download/1.0/MetalDAM_labeled.zip",
        "https://github.com/ari-dasci/OD-MetalDAM/releases/download/1.0/MetalDAM_metadata.sql",
    ]
    
    for url in urls:
        filename = url.split('/')[-1]
        print(f"Downloading {filename}...")
        
        response = requests.get(url)
        response.raise_for_status()
        
        with open(filename, 'wb') as f:
            f.write(response.content)
        
        print(f"Downloaded {filename}")

def extract_datasets():
    """
    Extract zip files to their down directory
    """
    
    zip_files = [
        "MetalDAM_labeled.zip",
        "Additive_unlabeled_publish.zip"
    ]
    
    for zip_file in zip_files:
        print(f"Extracting {zip_file}...")
        
        extract_dir = os.path.splitext(zip_file)[0]
        os.makedirs(extract_dir, exist_ok=True)
        
        with zipfile.ZipFile(zip_file, 'r') as zip_ref:
            zip_ref.extractall(extract_dir)
        
        print(f"Extracted {zip_file} to {extract_dir}/")

def sql_to_csv():
    """
    Parse MetalDAM_metadata.sql to csv for easier processing
    """
    
    # Connect to the SQLite database
    conn = sqlite3.connect("MetalDAM_metadata.sql")

    # Use read_sql_query instead of read_sql_table
    dataset = pd.read_sql_query("SELECT * FROM micrograph", conn)
    
    # Save to CSV
    dataset.to_csv("micrograph.csv", index=False)
    
    # Close the connection
    conn.close()
    
    print(f"Successfully exported {len(dataset)} rows to micrograph.csv")

def crop_images():
    """
    Crop images to remove the information band at the bottom.
    The labels are already cropped, so we only need to crop the actual images.
    """
    from PIL import Image
    
    # Read the metadata to get cropbar values
    metadata_df = pd.read_csv("micrograph.csv")
    
    # Setup directories
    images_input_dir = os.path.join("MetalDAM_labeled", "MetalDAM", "images")
    images_output_dir = os.path.join("MetalDAM_labeled", "MetalDAM", "images_cropped")
    os.makedirs(images_output_dir, exist_ok=True)
    
    print("Cropping images...")
    
    # Process each image
    for _, row in metadata_df.iterrows():
        image_filename = f"micrograph{row['micrograph_id']}.jpg"
        image_path = os.path.join(images_input_dir, image_filename)
        
        if not os.path.exists(image_path):
            print(f"Warning: Image {image_path} not found")
            continue
            
        # Open, crop, and save in one efficient operation
        with Image.open(image_path) as img:
            width, height = img.size
            cropped_img = img.crop((0, 0, width, height - row['cropbar']))
            
            # Verify dimensions match expected
            if cropped_img.size != (row['width'], row['height']):
                print(f"Warning: {image_filename} cropped to {cropped_img.size}, "
                      f"expected ({row['width']}, {row['height']})")
            
            # Save the cropped image
            cropped_img.save(os.path.join(images_output_dir, image_filename), quality=100)
    
    print(f"Cropped {len(metadata_df)} images to {images_output_dir}")
    print("Cropping complete!")


def parse_to_fiftyone():
    """
    Parse the MetalDAM dataset into FiftyOne format with segmentation masks and metadata.
    
    This function creates a FiftyOne dataset from the MetalDAM micrograph data by:
    1. Loading metadata from the CSV file generated by export_metadata()
    2. Creating FiftyOne samples with image paths, segmentation masks, and metadata
    3. Configuring the dataset with proper mask target labels
    4. Computing metadata and adding dynamic fields for the dataset
    
    Returns:
        fo.Dataset: A FiftyOne dataset containing all micrograph samples with their
                   associated segmentation masks and metadata (magnification, micron_bar,
                   pixel counts for each label class)
    
    Note:
        - Uses cropped images from images_cropped directory
        - Segmentation masks are loaded from the labels directory
        - Assumes micrograph.csv exists in the current directory
    """
    # Load the metadata CSV generated by export_metadata()
    metadata_df = pd.read_csv("micrograph.csv")
    
    # Define dataset directory structure
    images_dir = "MetalDAM_labeled/MetalDAM"
    image_path = os.path.join(images_dir, "images_cropped")  # Use cropped images
    mask_path = os.path.join(images_dir, "labels")           # Segmentation masks
    
    # Initialize list to store FiftyOne samples
    samples = []
    
    # Create FiftyOne samples for each micrograph
    for _, row in metadata_df.iterrows():
        micrograph_id = row['micrograph_id']
        
        # Create a FiftyOne sample with image, mask, and metadata
        sample = fo.Sample(
            # Path to the cropped micrograph image
            filepath=os.path.join(image_path, f"micrograph{micrograph_id}.jpg"),
            
            # Segmentation mask with absolute path for FiftyOne compatibility
            mask=fo.Segmentation(
                mask_path=os.path.abspath(
                    os.path.join(mask_path, f"micrograph{micrograph_id}.png")
                )
            ),
            
            # Microscopy metadata
            micron_bar=row['micron_bar'],           # Scale bar value in microns
            magnification=row['magnification'],     # Microscope magnification
            
            # Pixel counts for each segmentation class
            label0_pixels=row['label0'],            # Matrix pixels
            label1_pixels=row['label1'],            # Austenite pixels
            label2_pixels=row['label2'],            # Martensite/Austenite pixels
            label3_pixels=row['label3'],            # Precipitate pixels
            label4_pixels=row['label4'],            # Defect pixels
            total_pixels=row['total']               # Total image pixels
        )
        samples.append(sample)
    
    # Create and configure the FiftyOne dataset
    dataset = fo.Dataset(name="OD_MetalDAM", overwrite=True)
    
    # Define semantic labels for each mask value
    dataset.default_mask_targets = {
        0: "Matrix",                # Background matrix material
        1: "Austenite",            # Austenite phase
        2: "Martensite/Austenite", # Mixed martensite/austenite phase
        3: "Precipitate",          # Precipitate particles
        4: "Defect"                # Defects and artifacts
    }
    
    # Add samples to the dataset and compute metadata
    dataset.add_samples(samples)
    dataset.compute_metadata()        # Compute image dimensions, etc.
    dataset.add_dynamic_sample_fields()  # Enable dynamic field computation
    dataset.app_config.color_scheme = fo.ColorScheme(color_by="value")
    
    print(f"Created FiftyOne dataset with {len(samples)} samples")
    return dataset


def main():
    """
    Main function to execute the complete pipeline for processing MetalDAM dataset.
    """
    print("Starting MetalDAM dataset processing pipeline...")
    print("=" * 60)
    
    # Step 1: Download dataset if files don't exist
    if not os.path.exists("MetalDAM_labeled.zip") or not os.path.exists("MetalDAM_metadata.sql"):
        print("\nStep 1: Downloading dataset files...")
        download_dataset()
    else:
        print("\nStep 1: Dataset files already exist, skipping download.")
    
    # Step 2: Extract datasets
    if not os.path.exists("MetalDAM_labeled"):
        print("\nStep 2: Extracting dataset archives...")
        extract_datasets()
    else:
        print("\nStep 2: Dataset already extracted, skipping extraction.")
    
    # Step 3: Convert SQL metadata to CSV
    if not os.path.exists("micrograph.csv"):
        print("\nStep 3: Converting SQL metadata to CSV...")
        sql_to_csv()
    else:
        print("\nStep 3: CSV file already exists, skipping conversion.")
    
    # Step 4: Crop images
    images_cropped_dir = os.path.join("MetalDAM_labeled", "MetalDAM", "images_cropped")
    if not os.path.exists(images_cropped_dir) or len(os.listdir(images_cropped_dir)) == 0:
        print("\nStep 4: Cropping images...")
        crop_images()
    else:
        print("\nStep 4: Images already cropped, skipping cropping.")
    
    # Step 5: Parse to FiftyOne
    print("\nStep 5: Creating FiftyOne dataset...")
    dataset = parse_to_fiftyone()
    
    print("\n" + "=" * 60)
    print("Pipeline completed successfully!")
    print(f"FiftyOne dataset 'OD_MetalDAM' created with {len(dataset)} samples")
    print("\nTo visualize the dataset, run:")
    print("  import fiftyone as fo")
    print("  dataset = fo.load_dataset('OD_MetalDAM')")
    print("  session = fo.launch_app(dataset)")


if __name__ == "__main__":
    main()